{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import io\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "import jsonpickle\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import io\n",
    "import logging\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model.model import ResMLP\n",
    "from utils import enable_dropout, forward_mc, read_json\n",
    "\n",
    "from flask import Flask, request\n",
    "import logging\n",
    "from insightface.app.face_analysis import FaceAnalysis as FaceDetectionRecognition\n",
    "import numpy as np\n",
    "import argparse\n",
    "import io\n",
    "from PIL import Image\n",
    "from utils import resize_square_image, get_original_bbox, get_original_lm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Image to Estimate Age and Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 220\u001b[0m\n\u001b[1;32m    216\u001b[0m     cap\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    217\u001b[0m     cv2\u001b[39m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m--> 220\u001b[0m run_image(\u001b[39m'\u001b[39;49m\u001b[39mhttp://127.0.0.1:10002/\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mhttp://127.0.0.1:10003/\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    221\u001b[0m           \u001b[39m'\u001b[39;49m\u001b[39mtest_photos/group/test2.jpg\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[6], line 138\u001b[0m, in \u001b[0;36mrun_image\u001b[0;34m(url_face, url_age_gender, image_path)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(image_path, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m stream:\n\u001b[1;32m    136\u001b[0m     binary_image \u001b[39m=\u001b[39m stream\u001b[39m.\u001b[39mread()\n\u001b[0;32m--> 138\u001b[0m genders, ages, bboxes, det_scores, landmarks, embeddings \u001b[39m=\u001b[39m send_to_servers(\n\u001b[1;32m    139\u001b[0m     binary_image, url_face, url_age_gender\n\u001b[1;32m    140\u001b[0m )\n\u001b[1;32m    142\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(image_path)\n\u001b[1;32m    144\u001b[0m annotate_image(image, genders, ages, bboxes)\n",
      "Cell \u001b[0;32mIn[6], line 22\u001b[0m, in \u001b[0;36msend_to_servers\u001b[0;34m(binary_image, url_face, url_age_gender)\u001b[0m\n\u001b[1;32m     20\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mpost(url_face, json\u001b[39m=\u001b[39mdata)\n\u001b[1;32m     21\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgot \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m}\u001b[39;00m\u001b[39m from server!...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m response \u001b[39m=\u001b[39m jsonpickle\u001b[39m.\u001b[39;49mdecode(response\u001b[39m.\u001b[39;49mtext)\n\u001b[1;32m     24\u001b[0m face_detection_recognition \u001b[39m=\u001b[39m response[\u001b[39m\"\u001b[39m\u001b[39mface_detection_recognition\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     25\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(face_detection_recognition)\u001b[39m}\u001b[39;00m\u001b[39m faces deteced!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Downloads/temporary/age-gender-main/age-gender/lib/python3.10/site-packages/jsonpickle/unpickler.py:64\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(string, backend, context, keys, reset, safe, classes, v1_decode)\u001b[0m\n\u001b[1;32m     60\u001b[0m backend \u001b[39m=\u001b[39m backend \u001b[39mor\u001b[39;00m json\n\u001b[1;32m     61\u001b[0m context \u001b[39m=\u001b[39m context \u001b[39mor\u001b[39;00m Unpickler(\n\u001b[1;32m     62\u001b[0m     keys\u001b[39m=\u001b[39mkeys, backend\u001b[39m=\u001b[39mbackend, safe\u001b[39m=\u001b[39msafe, v1_decode\u001b[39m=\u001b[39mv1_decode\n\u001b[1;32m     63\u001b[0m )\n\u001b[0;32m---> 64\u001b[0m data \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39;49mdecode(string)\n\u001b[1;32m     65\u001b[0m \u001b[39mreturn\u001b[39;00m context\u001b[39m.\u001b[39mrestore(data, reset\u001b[39m=\u001b[39mreset, classes\u001b[39m=\u001b[39mclasses)\n",
      "File \u001b[0;32m~/Downloads/temporary/age-gender-main/age-gender/lib/python3.10/site-packages/jsonpickle/backend.py:210\u001b[0m, in \u001b[0;36mJSONBackend.decode\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoder_exceptions[name] \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m idx \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend_names) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 210\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    211\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    212\u001b[0m         \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/temporary/age-gender-main/age-gender/lib/python3.10/site-packages/jsonpickle/backend.py:207\u001b[0m, in \u001b[0;36mJSONBackend.decode\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mfor\u001b[39;00m idx, name \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend_names):\n\u001b[1;32m    206\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 207\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackend_decode(name, string)\n\u001b[1;32m    208\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoder_exceptions[name] \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    209\u001b[0m         \u001b[39mif\u001b[39;00m idx \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend_names) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/Downloads/temporary/age-gender-main/age-gender/lib/python3.10/site-packages/jsonpickle/backend.py:220\u001b[0m, in \u001b[0;36mJSONBackend.backend_decode\u001b[0;34m(self, name, string)\u001b[0m\n\u001b[1;32m    218\u001b[0m optargs, optkwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoder_options\u001b[39m.\u001b[39mget(name, ((), {}))\n\u001b[1;32m    219\u001b[0m decoder_kwargs \u001b[39m=\u001b[39m optkwargs\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m--> 220\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decoders[name](string, \u001b[39m*\u001b[39;49moptargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdecoder_kwargs)\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m     \u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[1;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "def send_to_servers(binary_image, url_face: str, url_age_gender: str) -> None:\n",
    "    \"\"\"Send a binary image to the two servers.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    binary_image: binary image\n",
    "    url_face: url of the face-detection-recognition server\n",
    "    url_age_gender: url of the age-gender server.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    genders, ages, bboxes, det_scores, landmarks, embeddings\n",
    "\n",
    "    \"\"\"\n",
    "    data = {\"image\": binary_image}\n",
    "    logging.info(f\"image loaded!\")\n",
    "\n",
    "    logging.debug(f\"sending image to server...\")\n",
    "    data = jsonpickle.encode(data)\n",
    "    response = requests.post(url_face, json=data)\n",
    "    logging.info(f\"got {response} from server!...\")\n",
    "    response = jsonpickle.decode(response.text)\n",
    "\n",
    "    face_detection_recognition = response[\"face_detection_recognition\"]\n",
    "    logging.info(f\"{len(face_detection_recognition)} faces deteced!\")\n",
    "\n",
    "    bboxes = [fdr[\"bbox\"] for fdr in face_detection_recognition]\n",
    "    det_scores = [fdr[\"det_score\"] for fdr in face_detection_recognition]\n",
    "    landmarks = [fdr[\"landmark\"] for fdr in face_detection_recognition]\n",
    "    embeddings = [fdr[\"normed_embedding\"]\n",
    "                  for fdr in face_detection_recognition]\n",
    "\n",
    "    # -1 accounts for the batch size.\n",
    "    data = np.array(embeddings).reshape(-1, 512).astype(np.float32)\n",
    "    data = pickle.dumps(data)\n",
    "    data = {\"embeddings\": data}\n",
    "    data = jsonpickle.encode(data)\n",
    "\n",
    "    logging.debug(f\"sending embeddings to server ...\")\n",
    "    response = requests.post(url_age_gender, json=data)\n",
    "    logging.info(f\"got {response} from server!...\")\n",
    "\n",
    "    response = jsonpickle.decode(response.text)\n",
    "    ages = response[\"ages\"]\n",
    "    genders = response[\"genders\"]\n",
    "\n",
    "    return genders, ages, bboxes, det_scores, landmarks, embeddings\n",
    "\n",
    "\n",
    "def annotate_image(image: Image.Image, genders: list, ages: list, bboxes: list) -> None:\n",
    "    \"\"\"Annotate a given image. This is done in-place. Nothing is returned.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    image: Pillow image\n",
    "    genders, ages, bboxes\n",
    "\n",
    "    \"\"\"\n",
    "    logging.debug(f\"annotating image ...\")\n",
    "\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    font = ImageFont.truetype(\"fonts/arial.ttf\", 25)\n",
    "\n",
    "    for gender, age, bbox in zip(genders, ages, bboxes):\n",
    "        draw.rectangle(bbox.tolist(), outline=(0, 0, 0))\n",
    "        draw.text(\n",
    "            (bbox[0], bbox[1]),\n",
    "            f\"AGE: {round(age['mean'])}, ENTROPY: {round(age['entropy'], 4)}\",\n",
    "            fill=(255, 0, 0),\n",
    "            font=font,\n",
    "        )\n",
    "        draw.text(\n",
    "            (bbox[0], bbox[3]),\n",
    "            \"MALE \" + str(round(gender[\"m\"] * 100)) + str(\"%\") + \", \"\n",
    "            \"FEMALE \"\n",
    "            + str(round(gender[\"f\"] * 100))\n",
    "            + str(\"%\")\n",
    "            + f\", ENTROPY: {round(gender['entropy'], 4)}\",\n",
    "            fill=(0, 255, 0),\n",
    "            font=font,\n",
    "        )\n",
    "\n",
    "\n",
    "def save_annotated_image(\n",
    "    image: Image.Image,\n",
    "    save_path: str,\n",
    "    bboxes: list,\n",
    "    det_scores: list,\n",
    "    landmarks: list,\n",
    "    embeddings: list,\n",
    "    genders: list,\n",
    "    ages: list,\n",
    ") -> None:\n",
    "    \"\"\"Save the annotated image.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    image: Pilow image\n",
    "    bboxes:\n",
    "    det_scores:\n",
    "    landmarks:\n",
    "    embeddings:\n",
    "    genders:\n",
    "    ages:\n",
    "\n",
    "    \"\"\"\n",
    "    image.save(save_path)\n",
    "    logging.info(f\"image annotated and saved at {save_path}\")\n",
    "\n",
    "    to_dump = {\n",
    "        \"bboxes\": bboxes,\n",
    "        \"det_scores\": det_scores,\n",
    "        \"landmarks\": landmarks,\n",
    "        \"embeddings\": embeddings,\n",
    "        \"genders\": genders,\n",
    "        \"ages\": ages,\n",
    "    }\n",
    "\n",
    "    with open(save_path + \".pkl\", \"wb\") as stream:\n",
    "        pickle.dump(to_dump, stream)\n",
    "    logging.info(f\"features saved at at {save_path + '.pkl'}\")\n",
    "\n",
    "\n",
    "def run_image(url_face: str, url_age_gender: str, image_path: str):\n",
    "    \"\"\"Run age-gender on the image.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    url_face: url of the face-detection-recognition server\n",
    "    url_age_gender: url of the age-gender server.\n",
    "    image_path\n",
    "\n",
    "    \"\"\"\n",
    "    logging.debug(f\"loading image ...\")\n",
    "    with open(image_path, \"rb\") as stream:\n",
    "        binary_image = stream.read()\n",
    "\n",
    "    genders, ages, bboxes, det_scores, landmarks, embeddings = send_to_servers(\n",
    "        binary_image, url_face, url_age_gender\n",
    "    )\n",
    "\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    annotate_image(image, genders, ages, bboxes)\n",
    "\n",
    "    save_path = image_path + \".ANNOTATED.jpg\"\n",
    "\n",
    "    save_annotated_image(\n",
    "        image, save_path, bboxes, det_scores, landmarks, embeddings, genders, ages\n",
    "    )\n",
    "\n",
    "\n",
    "def annotate_fps(image: Image.Image, fps: int) -> None:\n",
    "    \"\"\"Annotate fps on a given image.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    image: Pillow image\n",
    "    fps: frames per second\n",
    "\n",
    "    \"\"\"\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    font = ImageFont.truetype(\"fonts/arial.ttf\", 25)\n",
    "    draw.text((0, 0), f\"FPS: {fps} (Press q  to exit.)\",\n",
    "              fill=(0, 0, 255), font=font)\n",
    "\n",
    "\n",
    "def run_webcam(url_face: str, url_age_gender: str, camera_id: int):\n",
    "\n",
    "    import time\n",
    "\n",
    "    import cv2\n",
    "\n",
    "    cap = cv2.VideoCapture(camera_id)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Cannot open camera\")\n",
    "        exit()\n",
    "\n",
    "    # fps = []\n",
    "    while True:\n",
    "        start_time = time.time()  # start time of the loop\n",
    "        # Capture frame-by-frame\n",
    "        ret, image_BGR = cap.read()\n",
    "        # if frame is read correctly ret is True\n",
    "        if not ret:\n",
    "            print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "            break\n",
    "        # Our operations on the frame come here\n",
    "        # gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        # Display the resulting frame\n",
    "        image_RGB = cv2.cvtColor(image_BGR, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        image_PIL = Image.fromarray(image_RGB)\n",
    "        binary_image = io.BytesIO()\n",
    "        image_PIL.save(binary_image, format=\"JPEG\")\n",
    "        binary_image = binary_image.getvalue()\n",
    "\n",
    "        genders, ages, bboxes, det_scores, landmarks, embeddings = send_to_servers(\n",
    "            binary_image, url_face, url_age_gender\n",
    "        )\n",
    "\n",
    "        annotate_image(image_PIL, genders, ages, bboxes)\n",
    "\n",
    "        # fps.append(time)\n",
    "        fps = int(1.0 / (time.time() - start_time))\n",
    "\n",
    "        annotate_fps(image_PIL, fps)\n",
    "\n",
    "        cv2.imshow(\"frame\", cv2.cvtColor(\n",
    "            np.array(image_PIL), cv2.COLOR_RGB2BGR))\n",
    "        if cv2.waitKey(1) == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    # When everything done, release the capture\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "run_image('http://127.0.0.1:10002/', 'http://127.0.0.1:10003/',\n",
    "          'test_photos/group/test2.jpg')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('age-gender': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0c952b7deaf140ad60b711fd9f1151e1d2fae17e0c8f3f190ffdc33d4d244c59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
